from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from starlette.staticfiles import StaticFiles
import os
from openai import OpenAI
from docx import Document
import PyPDF2
import io
import json
import base64
import tempfile
import uuid
import re
from pathlib import Path
from typing import Optional

# åŠ è½½ .env æ–‡ä»¶
env_path = Path(__file__).parent.parent / '.env'
if env_path.exists():
    with open(env_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key.strip()] = value.strip()

app = FastAPI(title="æ–‡æ¡£æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨")

# åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç›®å½•
UPLOAD_DIR = Path(__file__).parent.parent / "uploads"
UPLOAD_DIR.mkdir(exist_ok=True)

# é…ç½®CORS - å…è®¸æ‰€æœ‰æ¥æºï¼ˆç”¨äºå±€åŸŸç½‘è®¿é—®ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æºï¼Œæ–¹ä¾¿å…¶ä»–è®¾å¤‡è®¿é—®
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™æ€æ–‡ä»¶æœåŠ¡
app.mount("/uploads", StaticFiles(directory=str(UPLOAD_DIR)), name="uploads")

# é…ç½®AI APIï¼ˆæ”¯æŒOpenAIå’ŒDeepSeekï¼‰
# ä¼˜å…ˆä½¿ç”¨DEEPSEEK_API_KEYï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨OPENAI_API_KEY
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

client = None
api_provider = None

if DEEPSEEK_API_KEY:
    # ä½¿ç”¨DeepSeek API
    client = OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url="https://api.deepseek.com"
    )
    api_provider = "deepseek"
    print("âœ… å·²é…ç½® DeepSeek API")
elif OPENAI_API_KEY:
    # ä½¿ç”¨OpenAI API
    client = OpenAI(api_key=OPENAI_API_KEY)
    api_provider = "openai"
    print("âœ… å·²é…ç½® OpenAI API")
else:
    print("âš ï¸  æœªé…ç½®API Keyï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

def extract_text_from_file(file_content: bytes, filename: str) -> str:
    """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
    if filename.endswith('.pdf'):
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    elif filename.endswith('.docx'):
        doc = Document(io.BytesIO(file_content))
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    elif filename.endswith('.txt'):
        return file_content.decode('utf-8')
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")

def generate_mindmap_with_llm(text: str) -> dict:
    """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ€ç»´å¯¼å›¾ç»“æ„ï¼ˆä¸­è‹±æ–‡ç‰ˆæœ¬ï¼‰"""
    prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„æ€ç»´å¯¼å›¾ã€‚

é‡è¦è¦æ±‚ï¼š
1. æ¯ä¸ªèŠ‚ç‚¹çš„åç§°å¿…é¡»ç®€æ´ï¼Œä¸­æ–‡ä¸è¶…è¿‡8ä¸ªå­—ï¼Œè‹±æ–‡ä¸è¶…è¿‡4ä¸ªå•è¯
2. å­èŠ‚ç‚¹åç§°è¦æ›´åŠ ç²¾ç‚¼ï¼Œä¸­æ–‡ä¸è¶…è¿‡6ä¸ªå­—ï¼Œè‹±æ–‡ä¸è¶…è¿‡3ä¸ªå•è¯
3. é¿å…ä½¿ç”¨é•¿å¥å­æˆ–æè¿°æ€§æ–‡å­—ï¼Œåªä½¿ç”¨å…³é”®è¯æˆ–çŸ­è¯­
4. éœ€è¦ç”Ÿæˆä¸­è‹±æ–‡ä¸¤ä¸ªç‰ˆæœ¬
5. **å…³é”®**ï¼šæ¯ä¸ªèŠ‚ç‚¹å¿…é¡»åŒ…å« "keywords" å­—æ®µï¼Œè¿™éå¸¸é‡è¦ï¼

å…³äº keywords å­—æ®µçš„è¦æ±‚ï¼š
- keywords å¿…é¡»æ˜¯ä»åŸæ–‡ä¸­**ç›´æ¥å¤åˆ¶**çš„è¯è¯­æˆ–çŸ­è¯­
- æ¯ä¸ªå…³é”®è¯åº”è¯¥æ˜¯åŸæ–‡ä¸­å®é™…å­˜åœ¨çš„ã€è¿ç»­çš„æ–‡æœ¬ç‰‡æ®µ
- é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„2-5ä¸ªå…³é”®è¯
- å…³é”®è¯åº”è¯¥èƒ½åœ¨åŸæ–‡ä¸­è¢«ç²¾ç¡®æ‰¾åˆ°
- ä¾‹å¦‚ï¼šå¦‚æœåŸæ–‡æœ‰"æœºå™¨å­¦ä¹ ç®—æ³•"ï¼Œå°±ç”¨"æœºå™¨å­¦ä¹ "æˆ–"ç®—æ³•"ä½œä¸ºå…³é”®è¯
- ä¸è¦åˆ›é€ æ–°è¯ï¼Œå¿…é¡»ä½¿ç”¨åŸæ–‡ä¸­çš„åŸè¯

æ€ç»´å¯¼å›¾åº”è¯¥ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "chinese": {{
    "name": "æ–‡æ¡£ä¸»é¢˜",
    "keywords": ["åŸæ–‡ä¸­çš„è¯1", "åŸæ–‡ä¸­çš„è¯2"],
    "children": [
      {{
        "name": "ç« èŠ‚1",
        "keywords": ["åŸæ–‡ä¸­çš„å…³é”®å¥"],
        "children": [
          {{"name": "è¦ç‚¹1", "keywords": ["åŸæ–‡ä¸­çš„è¯"]}},
          {{"name": "è¦ç‚¹2", "keywords": ["åŸæ–‡ä¸­çš„è¯"]}}
        ]
      }},
      {{
        "name": "ç« èŠ‚2",
        "keywords": ["åŸæ–‡å…³é”®è¯"],
        "children": [
          {{"name": "è¦ç‚¹3", "keywords": ["åŸæ–‡è¯è¯­"]}}
        ]
      }}
    ]
  }},
  "english": {{
    "name": "Topic",
    "keywords": ["word from text", "phrase from text"],
    "children": [
      {{
        "name": "Chapter 1",
        "keywords": ["exact text"],
        "children": [
          {{"name": "Point 1", "keywords": ["from original"]}},
          {{"name": "Point 2", "keywords": ["from text"]}}
        ]
      }},
      {{
        "name": "Chapter 2",
        "keywords": ["original word"],
        "children": [
          {{"name": "Point 3", "keywords": ["text snippet"]}}
        ]
      }}
    ]
  }}
}}

æ–‡æ¡£å†…å®¹ï¼š
{text[:3000]}

è¯·åªè¿”å›JSONæ ¼å¼çš„æ€ç»´å¯¼å›¾æ•°æ®ï¼ŒåŒ…å«chineseå’Œenglishä¸¤ä¸ªå­—æ®µã€‚
**é‡è¦**ï¼škeywords å¿…é¡»æ˜¯ä»ä¸Šé¢çš„æ–‡æ¡£å†…å®¹ä¸­ç›´æ¥æå–çš„åŸæ–‡ç‰‡æ®µï¼Œè¿™æ ·æ‰èƒ½åœ¨åŸæ–‡ä¸­é«˜äº®æ˜¾ç¤ºï¼"""

    try:
        if client:
            # æ ¹æ®APIæä¾›å•†é€‰æ‹©æ¨¡å‹
            if api_provider == "deepseek":
                model = "deepseek-chat"
            else:
                model = "gpt-3.5-turbo"
            
            print(f"ğŸ“¤ æ­£åœ¨è°ƒç”¨ {api_provider} API (æ¨¡å‹: {model})...")
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿å°†æ–‡æ¡£å†…å®¹ç»„ç»‡æˆæ€ç»´å¯¼å›¾ç»“æ„ã€‚è¯·ç¡®ä¿è¿”å›çš„JSONæ ¼å¼æ­£ç¡®ä¸”å®Œæ•´ï¼Œå¿…é¡»åŒ…å«chineseå’Œenglishä¸¤ä¸ªå­—æ®µã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=3000  # å¢åŠ tokené™åˆ¶ä»¥ç¡®ä¿å®Œæ•´è¿”å›
            )
            result = response.choices[0].message.content.strip()
            print(f"ğŸ“¥ AIåŸå§‹è¿”å› (å‰1000å­—ç¬¦): {result[:1000]}...")
            print(f"ğŸ“¥ AIåŸå§‹è¿”å›é•¿åº¦: {len(result)} å­—ç¬¦")
            
            # å°è¯•æå–JSON
            if "```json" in result:
                result = result.split("```json")[1].split("```")[0].strip()
            elif "```" in result:
                result = result.split("```")[1].split("```")[0].strip()
            
            # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡ï¼ˆå¯èƒ½åŒ…å«åœ¨æ–‡æœ¬ä¸­ï¼‰
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                result = json_match.group(0)
            
            # å°è¯•è§£æJSON
            try:
                parsed = json.loads(result)
                print(f"âœ… JSONè§£ææˆåŠŸ")
                return parsed
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"å°è¯•è§£æçš„å†…å®¹: {result[:200]}...")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤å¸¸è§é—®é¢˜
                try:
                    # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·ï¼ˆä½†è¦å°å¿ƒå­—ç¬¦ä¸²ä¸­çš„å•å¼•å·ï¼‰
                    result = result.replace("'", '"')
                    parsed = json.loads(result)
                    print(f"âœ… ä¿®å¤åJSONè§£ææˆåŠŸ")
                    return parsed
                except:
                    print(f"âŒ ä¿®å¤åä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„")
                    # è¿”å›é»˜è®¤ç»“æ„
                    return {
                        "chinese": {
                            "name": "æ–‡æ¡£åˆ†æç»“æœ",
                            "children": [
                                {"name": "JSONè§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥AIè¿”å›æ ¼å¼"}
                            ]
                        },
                        "english": {
                            "name": "Document Analysis Result",
                            "children": [
                                {"name": "JSON parsing failed, please check AI response format"}
                            ]
                        }
                    }
        else:
            # æ¨¡æ‹Ÿå“åº”ï¼ˆç”¨äºæ¼”ç¤ºï¼‰- åŒ…å«ä¸­è‹±æ–‡ç‰ˆæœ¬å’Œkeywords
            # ä»æ–‡æœ¬ä¸­æå–ä¸€äº›å…³é”®è¯ä½œä¸ºç¤ºä¾‹
            text_sample = text[:500] if text else ""
            return {
                "chinese": {
                    "name": "æ–‡æ¡£åˆ†æç»“æœ",
                    "keywords": text_sample.split()[:3],
                    "children": [
                        {
                            "name": "ç¬¬ä¸€ç« ï¼šæ¦‚è¿°",
                            "keywords": text_sample.split()[3:6] if len(text_sample.split()) > 6 else ["æ¦‚è¿°"],
                            "children": [
                                {"name": "èƒŒæ™¯ä»‹ç»", "keywords": text_sample.split()[6:9] if len(text_sample.split()) > 9 else ["èƒŒæ™¯"]},
                                {"name": "ç›®æ ‡è®¾å®š", "keywords": text_sample.split()[9:12] if len(text_sample.split()) > 12 else ["ç›®æ ‡"]}
                            ]
                        },
                        {
                            "name": "ç¬¬äºŒç« ï¼šä¸»è¦å†…å®¹",
                            "keywords": text_sample.split()[12:15] if len(text_sample.split()) > 15 else ["å†…å®¹"],
                            "children": [
                                {"name": "æ ¸å¿ƒæ¦‚å¿µ", "keywords": text_sample.split()[15:18] if len(text_sample.split()) > 18 else ["æ¦‚å¿µ"]},
                                {"name": "å®æ–½æ–¹æ³•", "keywords": text_sample.split()[18:21] if len(text_sample.split()) > 21 else ["æ–¹æ³•"]}
                            ]
                        },
                        {
                            "name": "ç¬¬ä¸‰ç« ï¼šæ€»ç»“",
                            "keywords": text_sample.split()[21:24] if len(text_sample.split()) > 24 else ["æ€»ç»“"],
                            "children": [
                                {"name": "å…³é”®è¦ç‚¹", "keywords": text_sample.split()[24:27] if len(text_sample.split()) > 27 else ["è¦ç‚¹"]},
                                {"name": "æœªæ¥å±•æœ›", "keywords": text_sample.split()[27:30] if len(text_sample.split()) > 30 else ["å±•æœ›"]}
                            ]
                        }
                    ]
                },
                "english": {
                    "name": "Document Analysis Result",
                    "keywords": text_sample.split()[:3],
                    "children": [
                        {
                            "name": "Chapter 1: Overview",
                            "keywords": text_sample.split()[3:6] if len(text_sample.split()) > 6 else ["overview"],
                            "children": [
                                {"name": "Background", "keywords": text_sample.split()[6:9] if len(text_sample.split()) > 9 else ["background"]},
                                {"name": "Objectives", "keywords": text_sample.split()[9:12] if len(text_sample.split()) > 12 else ["objectives"]}
                            ]
                        },
                        {
                            "name": "Chapter 2: Main Content",
                            "keywords": text_sample.split()[12:15] if len(text_sample.split()) > 15 else ["content"],
                            "children": [
                                {"name": "Core Concepts", "keywords": text_sample.split()[15:18] if len(text_sample.split()) > 18 else ["concepts"]},
                                {"name": "Implementation Methods", "keywords": text_sample.split()[18:21] if len(text_sample.split()) > 21 else ["methods"]}
                            ]
                        },
                        {
                            "name": "Chapter 3: Summary",
                            "keywords": text_sample.split()[21:24] if len(text_sample.split()) > 24 else ["summary"],
                            "children": [
                                {"name": "Key Points", "keywords": text_sample.split()[24:27] if len(text_sample.split()) > 27 else ["points"]},
                                {"name": "Future Outlook", "keywords": text_sample.split()[27:30] if len(text_sample.split()) > 30 else ["outlook"]}
                            ]
                        }
                    ]
                }
            }
    except Exception as e:
        # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„ç®€å•ç»“æ„
        print(f"âŒ APIè°ƒç”¨å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        # è¿”å›ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„ç®€å•ç»“æ„ï¼ˆä¸­è‹±æ–‡ç‰ˆæœ¬ï¼‰
        lines = [line.strip() for line in text.split('\n') if line.strip()][:20]
        return {
            "chinese": {
                "name": "æ–‡æ¡£å†…å®¹",
                "children": [{"name": line} for line in lines[:10]]
            },
            "english": {
                "name": "Document Content",
                "children": [{"name": line} for line in lines[:10]]
            }
        }

@app.get("/")
async def root():
    return {"message": "æ–‡æ¡£æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨API"}

@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£å¹¶ç”Ÿæˆæ€ç»´å¯¼å›¾"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        contents = await file.read()
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶ï¼ˆç”¨äºPDFæ˜¾ç¤ºï¼‰
        file_id = str(uuid.uuid4())
        file_ext = Path(file.filename).suffix
        saved_file_path = UPLOAD_DIR / f"{file_id}{file_ext}"
        
        with open(saved_file_path, "wb") as f:
            f.write(contents)
        
        # æå–æ–‡æœ¬
        text = extract_text_from_file(contents, file.filename)
        
        if not text.strip():
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
        
        # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ€ç»´å¯¼å›¾
        print(f"ğŸ¤– å¼€å§‹è°ƒç”¨AIç”Ÿæˆæ€ç»´å¯¼å›¾...")
        print(f"ğŸ“ æ–‡æ¡£æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        mindmap_data = generate_mindmap_with_llm(text)
        
        # è°ƒè¯•ï¼šæ‰“å°ç”Ÿæˆçš„æ€ç»´å¯¼å›¾æ•°æ®
        print(f"ğŸ“Š ç”Ÿæˆçš„æ€ç»´å¯¼å›¾æ•°æ®ç±»å‹: {type(mindmap_data)}")
        print(f"ğŸ“Š ç”Ÿæˆçš„æ€ç»´å¯¼å›¾æ•°æ®: {json.dumps(mindmap_data, ensure_ascii=False, indent=2)[:1000]}...")
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œç¡®ä¿æœ‰chineseæˆ–englishå­—æ®µï¼Œæˆ–è€…æœ‰nameå­—æ®µï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
        if not isinstance(mindmap_data, dict):
            print(f"âš ï¸  æ€ç»´å¯¼å›¾æ•°æ®æ ¼å¼é”™è¯¯: {type(mindmap_data)}")
            # å¦‚æœæ•°æ®æ ¼å¼ä¸å¯¹ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤ç»“æ„
            mindmap_data = {
                "chinese": {
                    "name": "æ–‡æ¡£åˆ†æç»“æœ",
                    "children": [{"name": "æ•°æ®æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥AIè¿”å›ç»“æœ"}]
                },
                "english": {
                    "name": "Document Analysis Result",
                    "children": [{"name": "Data format error, please check AI response"}]
                }
            }
        elif "chinese" not in mindmap_data and "english" not in mindmap_data:
            # å¦‚æœæ˜¯æ—§æ ¼å¼ï¼ˆç›´æ¥æœ‰nameå’Œchildrenï¼‰ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
            print("âš ï¸  æ£€æµ‹åˆ°æ—§æ ¼å¼æ•°æ®ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼")
            print(f"âš ï¸  æ—§æ ¼å¼æ•°æ®å†…å®¹: {json.dumps(mindmap_data, ensure_ascii=False, indent=2)[:500]}")
            if "name" in mindmap_data and "children" in mindmap_data:
                mindmap_data = {
                    "chinese": mindmap_data,
                    "english": mindmap_data  # æš‚æ—¶ä½¿ç”¨ç›¸åŒæ•°æ®
                }
            else:
                print(f"âš ï¸  æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œç¼ºå°‘nameæˆ–childrenå­—æ®µ")
                print(f"âš ï¸  å¯ç”¨å­—æ®µ: {list(mindmap_data.keys())}")
                # åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„é»˜è®¤ç»“æ„
                mindmap_data = {
                    "chinese": {
                        "name": "æ–‡æ¡£åˆ†æç»“æœ",
                        "children": [
                            {"name": "æ•°æ®æ ¼å¼ä¸å®Œæ•´"},
                            {"name": f"å¯ç”¨å­—æ®µ: {list(mindmap_data.keys())}"}
                        ]
                    },
                    "english": {
                        "name": "Document Analysis Result",
                        "children": [
                            {"name": "Incomplete data format"},
                            {"name": f"Available fields: {list(mindmap_data.keys())}"}
                        ]
                    }
                }
        
        # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿chineseå’Œenglishéƒ½æœ‰nameå­—æ®µ
        if "chinese" in mindmap_data:
            if not isinstance(mindmap_data["chinese"], dict) or "name" not in mindmap_data["chinese"]:
                print(f"âš ï¸  chineseæ•°æ®æ ¼å¼é”™è¯¯: {mindmap_data.get('chinese', {})}")
                mindmap_data["chinese"] = {
                    "name": "æ–‡æ¡£åˆ†æç»“æœ",
                    "children": [{"name": "æ•°æ®æ ¼å¼é”™è¯¯"}]
                }
        if "english" in mindmap_data:
            if not isinstance(mindmap_data["english"], dict) or "name" not in mindmap_data["english"]:
                print(f"âš ï¸  englishæ•°æ®æ ¼å¼é”™è¯¯: {mindmap_data.get('english', {})}")
                mindmap_data["english"] = {
                    "name": "Document Analysis Result",
                    "children": [{"name": "Data format error"}]
                }
        
        # è¿”å›æ–‡ä»¶URLï¼ˆç”¨äºå‰ç«¯è®¿é—®ï¼‰
        file_url = f"/uploads/{file_id}{file_ext}"
        
        return JSONResponse(content={
            "success": True,
            "filename": file.filename,
            "file_id": file_id,
            "file_url": file_url,
            "file_type": file_ext.lower(),
            "full_text": text,  # è¿”å›å®Œæ•´æ–‡æœ¬ï¼ˆç”¨äºéPDFæ–‡ä»¶ï¼‰
            "text_preview": text[:500] + "..." if len(text) > 500 else text,
            "mindmap": mindmap_data
        })
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

